#!/usr/bin/env python

'''Usage: transform_touches.py <initfile> (--frames <num_frames>) (--size <framesize>)

   Options:
         -f --frames    number of reading frames to open
         -s --size      size of each reading frame
'''


import docopt
import os, sys
from snap import snap
from snap import couchbasedbx as cbx
from snap import common
import tdxutils as tdx
import keygen
import constants as const
import logging
import datetime, time, arrow
import yaml
import couchbase

LOG_TAG = 'METL_transform'


def log_tag(pipeline_id):
    return '%s.%s' % (pipeline_id, LOG_TAG.lower())


def get_raw_records(segment_num, reading_frame, couchbase_pmgr, record_id_queue):
    range_start = reading_frame.index_number * reading_frame.size
    range_end = range_start + reading_frame.size - 1

    ids = record_id_queue.range(range_start, range_end)
    results = []
    for id in ids:
        raw_record = couchbase_pmgr.lookup_record_raw(id)
        results.append(cbx.CouchbaseRecordBuilder(const.RECTYPE_ORDER).add_fields(raw_record).build())

    return results


def transform(raw_record, pipeline_info_mgr, log):
    log.info('calling stubbed-out transform function in pipeline %s' % (pipeline_info_mgr.id))
    return cbx.CouchbaseRecordBuilder(const.RECTYPE_TOUCH_TRANSFORMED).add_fields(raw_record.__dict__).build()


def main(args):
    print tdx.jsonpretty(args)

    num_reading_frames = int(args.get('<num_frames>', -1))
    reading_frame_size = int(args.get('<framesize>', -1))
    segment = -1  
    initial_reading_frame = None
    paging_context = None
    if num_reading_frames > 0 and reading_frame_size > 0:
        paging_context = tdx.PagingContext(num_reading_frames, reading_frame_size)
        
    init_filename = args['<initfile>']
    yaml_config = common.read_config_file(init_filename)
    num_segments = int(yaml_config['globals'].get('num_segments', -1))
    pipeline_id = yaml_config['globals']['pipeline_id']
    job_id = tdx.generate_job_id('transform', pipeline_id)
    log_filename = tdx.generate_logfile_name(job_id)
    log = tdx.init_logging(log_tag(pipeline_id), log_filename, logging.INFO)

    service_objects = snap.initialize_services(yaml_config, log)
    so_registry = common.ServiceObjectRegistry(service_objects)
    couchbase_svc = so_registry.lookup('couchbase')
    couchbase_svc.data_manager.register_keygen_function(const.RECTYPE_TOUCH, keygen.generate_touch_key)
    couchbase_svc.data_manager.register_keygen_function(const.RECTYPE_TOUCH_TRANSFORMED, keygen.generate_transformed_touch_key)
    
    redis_svc = so_registry.lookup('redis')
    raw_record_id_queue = redis_svc.get_raw_record_queue(pipeline_id)
    transformed_record_id_queue = redis_svc.get_transformed_record_queue(pipeline_id)

    pipeline_info_mgr = tdx.PipelineInfoManager(pipeline_id,                                                
                                                couchbase_svc.journal_manager)

    job_dbkey = pipeline_info_mgr.record_job_start(job_id, args)

    print '%s script started at %s, logging to %s...' % (job_id, datetime.datetime.now().isoformat(), log_filename)
    log.info('jobid %s in pipeline %s started at %s' % (job_id, pipeline_id, datetime.datetime.now().isoformat()))
    start_time = time.time()

    
    paging_context = tdx.PagingContext(num_reading_frames, reading_frame_size)
    reading_frame = paging_context.initial_reading_frame()
    pages_remaining = paging_context.num_frames_to_read
    num_records_processed = 0
    num_transforms = 0

    while pages_remaining:
        raw_records = get_raw_records(segment, reading_frame, couchbase_svc.data_manager, raw_record_id_queue)
        for rec in raw_records:
            try:
                cooked_rec = transform(rec, pipeline_info_mgr, log)
                key = couchbase_svc.data_manager.insert_record(cooked_rec)
                transformed_record_id_queue.push(key)
                num_transforms += 1
            except Exception, err:
                log.error('%s thrown while transforming record: %s' % (err.__class__.__name__, err.message))
                log.error('offending record data: %s' % rec.__dict__)

            num_records_processed += 1

            if num_records_processed % 100000 == 0:
                log.info('+++ %d order records transformed.' % (num_records_processed))

        reading_frame = reading_frame.shift_right()
        pages_remaining -= 1
        
    end_time = time.time()
    log.info('%d records processed.' % (num_records_processed))
    log.info('%d records transformed.' % (num_transforms))
    log.info('Total running time %s' % (tdx.hms_string(end_time - start_time)))
    pipeline_info_mgr.record_job_end(job_dbkey)
    

    
if __name__=='__main__':
    args = docopt.docopt(__doc__)
    main(args)
